---
title: "svm"
author: "Standford University"
date: "July 28, 2019"
output: html_document
---

## SVM

To demonstrate the SVM, it is easiest to work in low dimensions,
so we can see the data.

Linear SVM classifier
----------------------

Lets generate some data in two dimensions, and make them a little separated.


```{r}
set.seed(10111)
x=matrix(rnorm(40),20,2)
y=rep(c(-1,1), c(10,10))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3, pch=19)
```

Now we wll load the package `e1071` which contains the `svm` function we will use. We then compute the fir. Notice that we have to specify a `cost` parameter, which is a tuning parameter.


```{r}
library(e1071)

dat=data.frame(x,y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)

print(svmfit)
plot(svmfit,dat)
```


As mentioned in the chapter, the plot function is somewhat crude, and plots X2 on the horizontal axis. Lets see how we might make our own plot.


```{r}
make.grid=function(x, n=75){
  grange=apply(x,2,range)
  x1=seq(from=grange[1,1],to=grange[2,1], length=n)
  x2=seq(from=grange[1,2],to=grange[2,2], length=n)
  expand.grid(X1=x1,X2=x2)
}

xgrid=make.grid(x)
ygrid=predict(svmfit, xgrid)
plot(xgrid, col=c("red", "blue")[as.numeric(ygrid)], pch=20,cex=.2)
points(x, col=y+3, pch=19)
points(x[svmfit$index,], pch=5, cex=2)

```


```{r}
beta=drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0=svmfit$rho
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)], pch=20, cex=.2)
points(x,col=y+3, pch=19)
points(x[svmfit$index,], pch=5, cex=2)
abline(beta0/beta[2], -beta[1]/beta[2])
abline((beta0-1)/beta[2], -beta[1]/beta[2], lty=2)
abline((beta0+1)/beta[2], -beta[1]/beta[2], lty=2)

```


Nonlinear SVM
------------------

Instead we will run the SVM on the same data wehere a non-linear boundary is called for. We will use the mixture data from ESL

```{r}
load(url("http://web.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"))

names(ESL.mixture)
rm(x,y)
attach(ESL.mixture)

plot(x,col=y+1)
dat=data.frame(y=factor(y),x)

fit=svm(factor(y)~., data=dat, scale=FALSE, kernel="radial", cost=5)
```

Now we are going to crate a grid, as before, and make predictions on the grid.
These data have the grid points for each variable included on the data frame.

```{r}
xgrid=expand.grid(X1=px1, X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid, col=as.numeric(ygrid), pch=20, cex=.2)
points(x, col=y+1, pch=19)
```

```{r}
func=predict(fit, xgrid, decision.values=TRUE)
func=attributes(func)$decision
xgrid=expand.grid(X1=px1, X2=px2)
ygrid=predict(fit, xgrid)
plot(xgrid, col=as.numeric(ygrid), pch=20, cex=.2)
points(x,col=y+1,pch=19)

contour(px1, px2, matrix(func, 69, 99), level=0, add=TRUE)
contour(px1,px2, matrix(prob,69,99), level=.5, add=TRUE, col="blue", lwd=2)
```

